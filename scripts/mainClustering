#===== Classification Tools =====#
from classes.commons.classification_functions import load_train_test_outlier_for_each_person
from classes.converters.hmp import HMPConverter
from classes.converters.arcma import ARCMAConverter
from classes.converters.umaAdl import UmaAdlConverter
#===== Machine Learn =====#
from sklearn.cluster import MeanShift, KMeans
#===== Utils =====#
import os
import numpy as np
from classes.commons.utils import print_verbose


##=== Function to Clustering the Dataset ===##
def clusteringWithMeanShift(dataset, cluster_num, filename, tablename, features, label_axis, label_column, window_len, person_indexes, person_column, additional_where = "",verbose = False):
    ##===Load Dataset===##
    list_train_features, list_train_labels, list_test_features, list_test_labels = load_train_test_outlier_for_each_person(dataset, filename, tablename, features, label_axis, label_column, window_len, person_indexes, person_column, additional_where)
    list_train_features = list_train_features[6]
    list_train_labels = list_train_labels[6]
    #cluster = MeanShift().fit(list_train_features)
    cluster = KMeans(n_clusters=cluster_num, random_state=170).fit(list_train_features)
    ##===Print number of activities===##
    print_verbose("Number of activities: {}".format(len(np.unique(list_train_labels))), verbose)
    unique_activities = np.unique(list_train_labels)
    proportion_aux = 0
    for activity in unique_activities:
        activity_indexes = np.where(list_train_labels == activity)[0]
        proportion = len(activity_indexes)/len(list_train_labels)
        proportion_aux = proportion_aux + proportion
        print_verbose("Activity {}, proportion: {}".format(activity, proportion), verbose)
    print_verbose("Sum of Proportion:{}".format(proportion_aux), verbose)
    ##===Print number of activities===##
    print_verbose("Number of clusters: {}".format(len(np.unique(list_train_labels))), verbose)
    ##===Verify labels for each cluster===##
    labels = cluster.labels_
    train_labels = list_train_labels
    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    return_cluster = []
    for l in range(0,n_clusters):
        indexes = np.where(labels == l)
        print_verbose("========= Cluster {} ========".format(l), verbose)
        train_labels_aux = train_labels[indexes]
        unique_activities_cluster = np.unique(train_labels_aux)
        aux_cluster = []
        for ua in unique_activities_cluster:
            ap = np.where(train_labels_aux == ua)
            p = (len(ap[0])/len(train_labels_aux))*100

            aux_cluster.append({"activity":ua, "proportion":p})

            print_verbose("Activities found: {} - {}%".format(ua, p), verbose)
        if len(aux_cluster) > 0:
            return_cluster.append(aux_cluster)
        print_verbose("---------------------------------", verbose)
    return return_cluster

##=== Searching Similar Activities To Remove ===##
def searching_similar_activities(clusters, verbose=False):
    ##== Searching Bigger Cluster ==##
    bigger_cluster = []
    for c in clusters:
        if len(c) > len(bigger_cluster):
            bigger_cluster = c
    ##== A) - Searching if any activity there are in all clusters, if yes, delete ==##
    activities_to_delete = []
    if len(bigger_cluster) > 0:
        for activity in bigger_cluster: # Verify all activities of the bigger cluster
            for c in clusters:
                occurences = c.count(activity)
                if occurences < 1: # Activitie doesn't exist in cluster
                    activity_in_all_cluters = False
            if activity_in_all_cluters:
                activities_to_delete.append(activity)

    ##== B) - Some activity there are in more that one cluster?
        activities_dic = {}
        for c in clusters:
            max_proportion = max(c, key=lambda x: x["proportion"])
            for activity in c:
                if list(activities_dic.keys()).count(activity['activity']) > 0:
                    activities_dic[activity['activity']]["count"] = activities_dic[activity['activity']]["count"] + 1
                    activities_dic[activity['activity']]["other_activities"].append({"len":(len(c)-1), "max_proportion":(activity==max_proportion)})
                else:

                    activities_dic[activity['activity']] = {"count": 1, "other_activities": [{"len":(len(c)-1), "max_proportion":(activity==max_proportion)}]}
        print_verbose("Activity SET({}) : {}".format(len(activities_dic.keys()),activities_dic), verbose)

        for activity in activities_dic.keys():
            activity_dic = activities_dic[activity]
            if activity_dic["count"] > 1: # Activity with more that one cluster
                min_len = min(activity_dic["other_activities"], key=lambda x: x["len"]) #
                if min_len["len"] > 0: # In all clusters, this activity appear together with other activities
                    max_proportion = list(map(lambda x: x["max_proportion"], activity_dic["other_activities"])) # The activity has no major proportion in any cluster
                    if not any(max_proportion):
                        activities_to_delete.append(activity)

    print_verbose("Activities to delete : {}".format(activities_to_delete), verbose)
    return activities_to_delete

def clusteringWithoutSimilarActivities(dataset, cluster_num, filename, tablename, features, label_axis, label_column, window_len, person_indexes, person_column, additional_where = "", verbose = False):
    clusters = clusteringWithMeanShift(dataset, cluster_num, filename, tablename, features, label_axis,label_column, window_len, person_indexes,person_column, additional_where=additional_where, verbose=True)
    activities_to_delete = searching_similar_activities(clusters, verbose)
    loop = 0
    if len(activities_to_delete) > 0:
        activities_to_delete_last_len = 0
        while(activities_to_delete_last_len < len(activities_to_delete)):
            activities_to_delete_last_len = len(activities_to_delete)
            additional_where_activity = ""
            for a in activities_to_delete:
                additional_where_activity = additional_where_activity + " and activity <> '{}'".format(a)
            print_verbose(additional_where_activity, verbose)
            clusters = clusteringWithMeanShift(dataset, cluster_num, filename, tablename, features, label_axis, label_column, window_len, person_indexes, person_column, additional_where=additional_where_activity, verbose=True)
            activities_to_delete = activities_to_delete + searching_similar_activities(clusters, verbose)

            print_verbose("Loop: {}".format(loop), verbose)
            print_verbose("AC TO DE: {} | AC TO DE LAST: {}".format(len(activities_to_delete), activities_to_delete_last_len), verbose)
            loop = loop + 1

    print_verbose("Total activities to Delete = {}".format(activities_to_delete), verbose)

##=== Calling The Functions ===##

##= HMP = ###
hmp = HMPConverter("")
person_indexes = ["'f1'", "'m1'", "'m2'", "'f2'", "'m3'", "'f3'", "'m4'", "'m5'", "'m6'", "'m7'", "'f4'", "'m8'", "'m9'", "'f5'", "'m10'", "'m11'", "'f1_1'", "'f1_2'", "'f1_3'", "'f1_4'", "'f1_5'", "'m1_1'", "'m1_2'", "'m2_1'", "'m2_2'", "'f3_1'", "'f3_2'"]
clusteringWithoutSimilarActivities(hmp, 14,"..\\hmp.db", "hmp", "x, y, z, activity", ["x","y","z"], "activity", 100, person_indexes, "person", verbose=True)

##= ARCMA = ##
#arcma = ARCMAConverter("..\\..\\\\databases\\arcma")
#clusteringWithoutSimilarActivities(arcma, 8, "..\\arcma.db", "arcma", "x, y, z, activity", ["x","y","z"], "activity", 100, list(range(1,16)), "person", verbose=True)

##= UMAFALL = ##
#umafall = UmaAdlConverter('')
#clusteringWithoutSimilarActivities(umafall, 11, "..\\umafall.db", "umafall", "XAxis, YAxis, ZAxis, activity", ["XAxis","YAxis","ZAxis"],"activity", 100, list(range(1,18)), "person", additional_where = "and SensorType=0 and SensorID=0",verbose=True)
